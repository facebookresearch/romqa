{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfc135f-c616-4335-a5ed-ba72a0091123",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "\n",
    "This notebook contains the steps to recreate the RoMQA database.\n",
    "Due to legal reasons, we (Meta) could not reproduce and host data from Wikidata and T-REX.\n",
    "Instead this notebook will combine Wikidata and T-REX into a database, and produce the final dataset by merging the database with crowd-source question annotations.\n",
    "\n",
    "PLEASE NOTE:\n",
    "Because Wikidata is constantly changing and they do not keep old dumps, it is plausible that by running this script on the newest Wikidata, you will *get a different result*.\n",
    "The way to avoid this is to get the linked database [from a third party](https://s3.us-west-1.wasabisys.com/vzhong-public/RoMQA/data.db.bz2).\n",
    "\n",
    "If you want to reproduce a database similar to that used to build RoMQA, please follow along.\n",
    "This takes several hours (~24hrs on a Macbook Pro 2020) to run.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. parse T-Rex data for evidence text, entities, and propositions.\n",
    "2. parse WikiData for entity and proposition metadata.\n",
    "3. merge T-Rex and WikiData entities, propositions, and evidence into database.\n",
    "\n",
    "Original T-REX link: https://hadyelsahar.github.io/t-rex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042237b7-7ad3-4bed-b614-7ad206ed426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qwikidata==0.4.1 in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: tqdm==4.59.0 in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (4.59.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from qwikidata==0.4.1->-r requirements.txt (line 1)) (2.25.1)\n",
      "Requirement already satisfied: mypy-extensions in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from qwikidata==0.4.1->-r requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from requests->qwikidata==0.4.1->-r requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from requests->qwikidata==0.4.1->-r requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from requests->qwikidata==0.4.1->-r requirements.txt (line 1)) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/lib/python3.8/site-packages (from requests->qwikidata==0.4.1->-r requirements.txt (line 1)) (2020.12.5)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3cfdee-b79c-4cca-a02b-7dc606c19c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-01 11:52:50--  https://figshare.com/ndownloader/files/8760241\n",
      "Resolving figshare.com (figshare.com)... 34.248.76.93, 52.210.169.218\n",
      "Connecting to figshare.com (figshare.com)|34.248.76.93|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/8760241/TREx.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20221101/eu-west-1/s3/aws4_request&X-Amz-Date=20221101T185251Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=5ade19d3dfd15fd916326abe6eefdfe116a0a497645d0f2aa0c4c2fdaef0dbad [following]\n",
      "--2022-11-01 11:52:51--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/8760241/TREx.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20221101/eu-west-1/s3/aws4_request&X-Amz-Date=20221101T185251Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=5ade19d3dfd15fd916326abe6eefdfe116a0a497645d0f2aa0c4c2fdaef0dbad\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.92.34.184, 52.218.117.128, 52.92.32.40, ...\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.92.34.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4768765975 (4.4G) [binary/octet-stream]\n",
      "Saving to: ‘trex.zip’\n",
      "\n",
      "trex.zip            100%[===================>]   4.44G  2.32MB/s    in 27m 17s \n",
      "\n",
      "2022-11-01 12:20:09 (2.78 MB/s) - ‘trex.zip’ saved [4768765975/4768765975]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc -O trex.zip https://figshare.com/ndownloader/files/8760241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c3e58e-45e4-40f7-a660-58a902f9f5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ddbe3159c34b329b7eaa7e85587809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import zipfile\n",
    "import ujson as json\n",
    "from tqdm import auto as tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "ftrex = 'trex.zip'\n",
    "\n",
    "def create_triplet(subj, pred, obj):\n",
    "    return (subj['uri'], pred['uri'], obj['uri'])\n",
    "\n",
    "evidence = defaultdict(set)\n",
    "docs = {}\n",
    "with zipfile.ZipFile(ftrex) as fz:\n",
    "    bar = tqdm.tqdm(fz.filelist)  # there are 465 files\n",
    "    for fname in bar:\n",
    "        with fz.open(fname) as f:\n",
    "            data = json.load(f)\n",
    "            for doc in data:\n",
    "                if doc['docid'] in docs:\n",
    "                    assert doc['text'] == docs[doc['docid']]['text']\n",
    "                else:\n",
    "                    docs[doc['docid']] = dict(title=doc['title'], text=doc['text'], uri=doc['uri'])\n",
    "                for t in doc['triples']:\n",
    "                    trip = create_triplet(t['subject'], t['predicate'], t['object'])\n",
    "                    sent_start, sent_end = doc['sentences_boundaries'][t['sentence_id']]\n",
    "                    evidence[trip].add((doc['docid'], sent_start, sent_end))\n",
    "        bar.set_description('{} docs, {} triplets'.format(len(docs), len(evidence)))\n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7278920-38d3-4498-875d-c16991e2a5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving docs\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p checkpoint\n",
    "\n",
    "import pickle\n",
    "print('Saving docs')\n",
    "with open('checkpoint/docs.pkl', 'wb') as f:\n",
    "    pickle.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be10278-31a6-47a2-9aee-c565403c5c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evidence\n"
     ]
    }
   ],
   "source": [
    "print('Saving evidence')\n",
    "with open('checkpoint/evidence.pkl', 'wb') as f:\n",
    "    pickle.dump(evidence, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7dbe7-8743-465d-8b18-d63cc3b9c8de",
   "metadata": {},
   "source": [
    "## Wikidata\n",
    "\n",
    "The dump is available from https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7104ee52-b41b-4579-9690-2336378d5583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-01 14:31:15--  https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.142\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 78333844049 (73G) [application/octet-stream]\n",
      "Saving to: ‘wikidata.json.bz2’\n",
      "\n",
      "wikidata.json.bz2   100%[===================>]  72.95G  4.56MB/s    in 5h 19m  \n",
      "\n",
      "2022-11-01 19:50:59 (3.89 MB/s) - ‘wikidata.json.bz2’ saved [78333844049/78333844049]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc -O wikidata.json.bz2 https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0827582-76d7-47b6-ad7e-e68b899f17d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc730d215b6144c8a195da1a7f76aefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import ujson as json\n",
    "from qwikidata.json_dump import WikidataJsonDump\n",
    "from qwikidata.entity import WikidataItem, WikidataProperty\n",
    "\n",
    "\n",
    "known_entities = set()\n",
    "known_props = set()\n",
    "for subj, prop, obj in evidence.keys():\n",
    "    known_entities.add(subj)\n",
    "    known_entities.add(obj)\n",
    "    known_props.add(prop)\n",
    "    \n",
    "    \n",
    "wjd = WikidataJsonDump('wikidata.json.bz2')\n",
    "type_to_entity_class = {\"item\": WikidataItem, \"property\": WikidataProperty}\n",
    "\n",
    "\n",
    "wikidata_entities = {}\n",
    "bar = tqdm.tqdm(wjd, total=None)\n",
    "for entity_dict in bar:\n",
    "    entity_id = 'http://www.wikidata.org/entity/{}'.format(entity_dict[\"id\"])\n",
    "    entity_type = entity_dict[\"type\"]\n",
    "    entity = type_to_entity_class[entity_type](entity_dict)\n",
    "    \n",
    "    if entity_id not in known_entities:\n",
    "        continue\n",
    "\n",
    "    if isinstance(entity, WikidataItem):\n",
    "        d = dict(\n",
    "            id=entity.entity_id,\n",
    "            label=entity.get_label(),\n",
    "            description=entity.get_description(),\n",
    "            aliases=entity.get_aliases(),\n",
    "            wikipedia_title=entity.get_enwiki_title(),\n",
    "        )\n",
    "        wikidata_entities[entity_id] = d\n",
    "        bar.set_description('{} matched'.format(len(wikidata_entities)))\n",
    "bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62790731-e150-43d3-a995-c0a6b56a6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_props = {}\n",
    "with bz2.open('props.json.bz2', 'rt') as f:\n",
    "    for prop_id, meta in json.load(f).items():\n",
    "        prop_id = 'http://www.wikidata.org/prop/direct/{}'.format(prop_id)\n",
    "        if prop_id in known_props:\n",
    "            wikidata_props[prop_id] = meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536040c7-b46a-450f-bb12-e95c66346a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving wikidata entities\n"
     ]
    }
   ],
   "source": [
    "print('Saving wikidata entities')\n",
    "with open('checkpoint/wikidata_entities.pkl', 'wb') as f:\n",
    "    pickle.dump(wikidata_entities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10618b-5bba-4691-b614-37c21900ede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving wikidata propositions\n"
     ]
    }
   ],
   "source": [
    "print('Saving wikidata propositions')\n",
    "with open('checkpoint/wikidata_props.pkl', 'wb') as f:\n",
    "    pickle.dump(wikidata_props, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e47526-c27d-475a-b524-73c305c0024f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006aa6c1-a11c-48c3-bca8-69c18bba0b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading evidence\n",
      "loading entities\n",
      "loading props\n",
      "loading docs\n",
      "T-Rex\n",
      "3075119 entities, 685 props, 6562805 triplets, 4645090 docs\n",
      "WikiData linked\n",
      "2940899 entities, 676 props\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print('loading evidence')\n",
    "with open('checkpoint/evidence.pkl', 'rb') as f:\n",
    "    evidence = pickle.load(f)\n",
    "\n",
    "known_entities = set()\n",
    "known_props = set()\n",
    "for subj, prop, obj in evidence.keys():\n",
    "    known_entities.add(subj)\n",
    "    known_entities.add(obj)\n",
    "    known_props.add(prop)\n",
    "\n",
    "print('loading entities')\n",
    "with open('checkpoint/wikidata_entities.pkl', 'rb') as f:\n",
    "    wikidata_entities = pickle.load(f)\n",
    "    \n",
    "print('loading props')\n",
    "with open('checkpoint/wikidata_props.pkl', 'rb') as f:\n",
    "    wikidata_props = pickle.load(f)\n",
    "    \n",
    "print('loading docs')\n",
    "with open('checkpoint/docs.pkl', 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "    orig_num_docs = len(docs)\n",
    "\n",
    "print('T-Rex')\n",
    "print('{} entities, {} props, {} triplets, {} docs'.format(len(known_entities), len(known_props), len(evidence), len(docs)))\n",
    "print('WikiData linked')\n",
    "print('{} entities, {} props'.format(len(wikidata_entities), len(wikidata_props)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e0c0b-8d38-47be-9927-28ef3d2d1fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "insert ents: 100%|██████████| 30/30 [00:19<00:00,  1.57it/s]\n",
      "insert props: 100%|██████████| 1/1 [00:00<00:00, 172.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import db_utils as D\n",
    "import sqlite3\n",
    "fdb = 'data.db'\n",
    "\n",
    "db = sqlite3.connect(fdb, isolation_level=None)\n",
    "D.make_tables(db, ['ents', 'props', 'docs', 'trips', 'evidence'])\n",
    "\n",
    "\n",
    "id2prop, prop2id = [], {}\n",
    "id2ent, ent2id = [], {}\n",
    "\n",
    "sorted_entities = sorted(list(wikidata_entities.keys()))\n",
    "for i, uri in enumerate(sorted_entities):\n",
    "    meta = wikidata_entities[uri]\n",
    "    x = i, uri, meta['label'], meta.get('aliases', ''), meta.get('description', ''), meta['wikipedia_title']\n",
    "    if uri not in ent2id:\n",
    "        ent2id[uri] = len(id2ent)\n",
    "        id2ent.append(x)\n",
    "\n",
    "sorted_props = sorted(list(wikidata_props.keys()))\n",
    "for i, uri in enumerate(sorted_props):\n",
    "    meta = wikidata_props[uri]\n",
    "    x = i, uri, meta['label'], meta.get('aliases', ''), meta.get('description', '')\n",
    "    if uri not in prop2id:\n",
    "        prop2id[uri] = len(id2prop)\n",
    "        id2prop.append(x)\n",
    "      \n",
    "      \n",
    "D.batch_insert(db, 'ents', id2ent)\n",
    "D.batch_insert(db, 'props', id2prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0bb2ad-f85f-4ace-b3f4-6d101366b086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f545d9c18f934b958c509aed658489a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6562805 [00:19<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruned triples from 6562805 to 5376817\n",
      "pruned evidence from 16117800 to 14829349\n",
      "loading docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "insert trips:   0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruned docs from 4645090 to 3347899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "insert trips: 100%|██████████| 54/54 [00:20<00:00,  2.64it/s]\n",
      "insert evidence: 100%|██████████| 149/149 [00:54<00:00,  2.73it/s]\n",
      "insert docs: 100%|██████████| 34/34 [03:29<00:00,  6.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import auto as tqdm\n",
    "\n",
    "\n",
    "id2trip, id2evidence = [], []\n",
    "seen_docs = set()\n",
    "seen_evidence = set()\n",
    "\n",
    "\n",
    "sorted_docs = sorted(list(docs.keys()))\n",
    "id2doc, doc2id = [], {}\n",
    "for uri in sorted_docs:\n",
    "    doc2id[uri] = len(id2doc)\n",
    "    id2doc.append(docs[uri])\n",
    "\n",
    "\n",
    "sorted_trips = sorted(list(evidence.keys()))\n",
    "orig_num_evidence = 0\n",
    "for i, trip in enumerate(tqdm.tqdm(sorted_trips)):\n",
    "    subj, prop, obj = trip\n",
    "    orig_num_evidence += len(evidence[trip])\n",
    "    if subj not in ent2id or obj not in ent2id or prop not in prop2id:\n",
    "        continue\n",
    "    subj_id, prop_id, obj_id = ent2id[subj], prop2id[prop], ent2id[obj]\n",
    "    x = i, subj_id, obj_id, prop_id\n",
    "    id2trip.append(x)\n",
    "  \n",
    "    for docid, start, end in evidence[trip]:\n",
    "        x = len(id2evidence), i, doc2id[docid], start, end\n",
    "        if (i, doc2id[docid], start, end) in seen_evidence:\n",
    "            continue\n",
    "        seen_evidence.add((i, doc2id[docid], start, end))\n",
    "        id2evidence.append(x)\n",
    "        seen_docs.add(doc2id[docid])\n",
    "print('pruned triples from {} to {}'.format(len(evidence), len(id2trip)))        \n",
    "print('pruned evidence from {} to {}'.format(orig_num_evidence, len(id2evidence)))        \n",
    "\n",
    "\n",
    "print('loading docs')\n",
    "id2doc = [(i, d['uri'], d['title'], d['text']) for i, d in enumerate(id2doc) if i in seen_docs]\n",
    "print('pruned docs from {} to {}'.format(orig_num_docs, len(id2doc)))\n",
    "D.batch_insert(db, 'trips', id2trip)\n",
    "D.batch_insert(db, 'evidence', id2evidence)\n",
    "D.batch_insert(db, 'docs', id2doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b9746-7f4a-462a-a560-8e424c2cd711",
   "metadata": {},
   "source": [
    "# Statistics for raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d730b10-9bf0-40f6-9c1e-807b256e49d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "235b5ebe-9271-458f-93e8-ad15e7d1f1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ents</td>\n",
       "      <td>2940899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>props</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trips</td>\n",
       "      <td>5376817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evidence</td>\n",
       "      <td>14829349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>docs</td>\n",
       "      <td>3347899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name     count\n",
       "0      ents   2940899\n",
       "1     props       676\n",
       "2     trips   5376817\n",
       "3  evidence  14829349\n",
       "4      docs   3347899"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_size(table):\n",
    "    q = 'SELECT COUNT(*) FROM {}'.format(table)\n",
    "    return db.execute(q).fetchone()[0]\n",
    "    \n",
    "counts = [dict(name=k, count=get_size(k)) for k in ['ents', 'props', 'trips', 'evidence', 'docs']]\n",
    "counts = pd.DataFrame(counts)\n",
    "counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
